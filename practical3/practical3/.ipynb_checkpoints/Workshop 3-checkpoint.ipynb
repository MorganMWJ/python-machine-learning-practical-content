{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation, and Kaggle Datasets\n",
    "\n",
    "### By Keiron O'Shea, and Chuan Lu\n",
    "\n",
    "In machine learning, we must assure that our model is able to disseminate correct features that can work with \"real data\". One means of ensuring that our models do this is through the use of cross-validation.\n",
    "\n",
    "Cross-validation is a technique in which a model is trained rusing a subset of the dataset, and then evaluated through the use of a (usually smaller) subset of the data.\n",
    "\n",
    "This involves three steps:\n",
    "\n",
    "1. Split the data into portions (training, validation, and testing) data.\n",
    "2. Use the training and validation data to train the model.\n",
    "3. Use the testing data to evaluate the model performance.\n",
    "\n",
    "For example given a dataset of 10 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([14, 12, 25, 7, 5, 17, 47, 52, 26, 69])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the data into two datasets, where 20% of the data is taken forward for testing purposes, leaving the remaining 8 samples to be used for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = X[0:8]\n",
    "testing_data = X[8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called validation, and is an extremely simplistic way of evaluating a model. However one issue with this approach is that it fails to provide any real metric of how the model performs against the entirety of the dataset.\n",
    "\n",
    "One way of ensuring that the dataset is well represented when evaluating our models would be to iterate through, whilst spliting the data.\n",
    "\n",
    "**Leave One Out Cross Validation** is a simple way of doing exactly this. In this method, we perform training on the entire dataset - only leaving one example of the available dataset out for testing purposes.\n",
    "\n",
    "So using the simple example above, we do this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X_train = X[[x for x in range(len(X)) if x != i]]\n",
    "    # y_train ....\n",
    "    X_test = X[i]\n",
    "    print(\"Training the model with %i samples, and %s tests\" % (len(X_train), 1))\n",
    "    # y_test ....\n",
    "    # Train model\n",
    "    # Add y_train to y_true\n",
    "    # Add prediction from X_test to y_pred\n",
    "    \n",
    "# Run metrics here (classification_report, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major issue of this method is that it may lead to a higher variation in the testing model. Another issue is that it may take a lot of execution time if the dataset is of large size - or if the model takes awhile to learn.\n",
    "\n",
    "**K-Fold Cross Validation** provides a superior representation of the dataset. In this method we split the dataset into k-number of subsets (folds), performing the training on the remaining dataset and evaluating on the fold.\n",
    "\n",
    "For example, given 10 samples:\n",
    "\n",
    "![title](images/dataset.png)\n",
    "\n",
    "We could set the K-parameter to equal 2 fold. This will effectively split the data into two equal sizes (where green is for training, and red is for testing):\n",
    "\n",
    "![title](images/fold1.png)\n",
    "\n",
    "Once evaluated, we then split the data again and evaluate the performance once more:\n",
    "\n",
    "![title](images/fold2.png)\n",
    "\n",
    "\n",
    "So in code format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(2)\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    print(\"\\nTraining the model with a dataset of %i length\" % len(train))\n",
    "    print(\"Test the model with a dataset of %i length\" % len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If need want to adjust the number of folds to help figure out how this works.\n",
    "\n",
    "### AUC - ROC Curve\n",
    "\n",
    "We've already played around with performance metrics in previous workshops, but we've not had any chance to play around with how we can visualise said results effectively.\n",
    "\n",
    "But before we do this, we must first venture into the wonderfully simplistic, yet incredibly powerful **Confusion Matrix**. The Confusion Matrix is a performance measurement for machine learning classification. In the simplest terms, it is comprised of a single table with 4 combinations of predicted and actual values. A nice example I found online was the pregnancy anology:\n",
    "\n",
    "![title](images/preggo.png)\n",
    "\n",
    "- **True Positive/TP:** You have predicted positive, and that is indeed the case.\n",
    "- **True Negative/TN:** You have predicted negative, amd that is indeed the case.\n",
    "- **False Positive/FP/Type 1 Error:** You have predicted positive, where the case is actually negative.\n",
    "- **False Negative/FN/Type 2 Error:** You have predicted negative, where the case is actually positive.\n",
    "\n",
    "Using this simple metric, we are able to calculate an array of performance metrics. For example:\n",
    "\n",
    "$$Recall = {TP \\over{TP + FN}}$$\n",
    "\n",
    "Which calculates how many classes were correctly predicted for the positive task.\n",
    "\n",
    "$$Precision = {TP \\over{TP + FP}}$$\n",
    "\n",
    "Which calculates how many classes were correctly predicted for the negative task.\n",
    "\n",
    "$$F-measure = {2*(Recall*Precision) \\over{Recall + Precision}}$$\n",
    "\n",
    "Uses the harmonic mean to punish the extreme values more, where two models with low precision and high recall (or vice versa).\n",
    "\n",
    "\n",
    "The **Area Under the Cuve** (AUC) and **Reciever Operating Characteristic** (ROC) curve are two of the most important evaluation metrics for checking any classification model's true predicitive performance.\n",
    "\n",
    "The major calculations are the **true positive rate**:\n",
    "\n",
    "$$TPR = {TP \\over{TP+FN}}$$\n",
    "\n",
    "The **false positive rate**:\n",
    "\n",
    "$$FPR = {FP \\over{FP+FP}}$$\n",
    "\n",
    "And the **specificity**:\n",
    "\n",
    "$$Specificity = {TN \\over{TN+FP}}$$\n",
    "\n",
    "The best possible model has an AUC of 1, which means that it has a good mesaure of seperability. 0 means that the model has a worst measure of seperability, and where AUC is 0.5 it illustrates that the model has zero class seperation capacity whatsoever.\n",
    "\n",
    "The code to calculate and plot the AUC and ROC in python can be found below:\n",
    "\n",
    "**Note:** This only works for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_auc_and_roc(probabilities, y_true):\n",
    "    fpr, tpr, _ = roc_curve(y_true, probabilities)\n",
    "    calc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    plt.title(\"Recieving Operating Characteristic\")\n",
    "    plt.plot(fpr, tpr, \"red\", label=\"AUC %0.2f\" % calc_auc)\n",
    "    plt.plot([0, 1], [0, 1], \"b--\")\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, over to you\n",
    "\n",
    "A major part of these workshops are to help you go out and do this for yourself. That being said, we do expect you to achieve the following:\n",
    "\n",
    "- Load in the data into a format that a machine learning technique can use.\n",
    "- Use models and techniques from previous workshops to build a model.\n",
    "- Evaluate the model using an appropriate form of cross validation (simple validation **will not suffice**).\n",
    "- Make use of the ROC function above to illustrate model performance.\n",
    "\n",
    "\n",
    "Register for Kaggle using your Aberystwyth University, using your AU email address.\n",
    "\n",
    "https://www.kaggle.com\n",
    "\n",
    "Log in and join the following competition:\n",
    "\n",
    "https://www.kaggle.com/c/csm6420-bbbp/\n",
    "\n",
    "**REMEMBER:** IT IS OK NOT TO KNOW, IF YOU ARE STRUGGLING TO MAKE A START ON THIS PROJECT, CALL OVER A DEMONSTRATOR FOR ASSISTANCE. MAKE ME EARN MY Â£8 A HOUR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
