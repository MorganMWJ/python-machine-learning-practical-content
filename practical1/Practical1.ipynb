{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A comfortable introduction to Machine Learning\n",
    "\n",
    "##### By Keiron O'Shea (keo7), and Chuan Lu (cul)\n",
    "\n",
    "As a means of easing you into the module and the python ecosystem in general, we will explore basic classification techniques through the use of decision trees.\n",
    "\n",
    "## Supervised and Unsupervised Learning\n",
    "\n",
    "Supervised learning describes the technique in which a machine learning model is built throgh the use of labeled training data. For example, if we wanted to build a model to predict as to whether an image **is** a hot dog or **not** a hot dog we will create a database of pictures with labels corresponding to each image detailing as to whether or not it is or is not a hot dog. When trained, the algorithm will learn how to determine whether or not a hotdog is visible in the image.\n",
    "\n",
    "![Supervised Learning](images/supervised_classification.png)\n",
    "\n",
    "**Figure One:** An example of a supervised classification task, in which the training examples in the orange segment are pre-labeled as being \"hot dog\", and those in the white segment are pre-labeled as being \"not a hot dog\".\n",
    "\n",
    "Unsupervised learning focuses on the building of machine learning models without the use of labeled data. As there are no labels available, the model will be required to extract nuances based on the data provided. This is very useful in areas in which we are unable to give 100% confirmation of a stratification, providing a useful tool to extract features that would have otherwise gone unnoticed.\n",
    "\n",
    "In this practical we will focus on supervised classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task One: Would you survive the Titanic?\n",
    "\n",
    "On April 15th 1912, during her maiden voyage, the RMS Titanic sank after a collision with an iceberg - killing 1,502 of the 2,224 passengers and crew on board. One of the main reasons as to why the deathtoll was so high was due to a lack of lifeboats.\n",
    "\n",
    "To get you up and running with the ecosystem, we ask you to complete the analysis of what \"class\" of person were likely to survive.\n",
    "\n",
    "#### Loading in the data\n",
    "\n",
    "In this directory, you will find a file named ```titanic.csv``` in the ```data``` directory. Open it up with Libre/Microsoft Office and study the data carefully. Each column heading variable has the following meaning:\n",
    "\n",
    "- ```survival```: Whether or not the passenger surprived or not (0 = False, 1 = True)\n",
    "- ```class```: Travel class of passenger (1 = First, 2 = Second...)\n",
    "- ```name```: Name of the passenger\n",
    "- ```sex```: Sex of the passenger\n",
    "- ```age```: Age of the passenger, in years\n",
    "- ```sibsp```: Number of siblings/spouses aboard\n",
    "- ```parch```: Number of parents/children aboard\n",
    "- ```ticket```: Ticket number of passenger\n",
    "- ```fare```: Fare paid by passenger\n",
    "- ```cabin```: Cabin in which the passenger stayed\n",
    "- ```embarked```: Port of emarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "- ```boat``` Lifeboat (if Survival == 1)\n",
    "- ```body```: Body number (if Survival == 0, and body recovered)\n",
    "\n",
    "To load this data into this notebook, we will make use of ```pandas```. ```pandas``` \"is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language\". In this practical, we will guide you through the use of this package - but in the future we do expect you to make use of the package's documentation. This can be found here:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/\n",
    "\n",
    "As the data is in the form of a comma-seperated value (```csv```) file, we will make use of ```pandas```' ```read_csv``` function. Documentation for this can be found here:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "\n",
    "Before we do this, we must first load the library into our project. We can do this using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Pandas\n",
    "Pandas is a Python library that contains high-level data structures and manipulation tools designed for data analysis. Think of Pandas as a Python version of Excel. Scikit-learn, on the other hand, is an open-source machine learning library for Python. While Scikit-learn does a lot of the heavy lifting, what's equally important is ensuring that raw data is processed in such a way that we are able to 'feed' it to Scikit-learn. Hence, the ability to manipulate raw data with Pandas makes it an indispensible part of our toolkit\n",
    "\n",
    "Whilst this is an acceptable way of loading in the library, when working with large projects it can be a bit tiresome to write ```pandas``` in full every time you are required to leverage on the library. Fortunately for us, we can make use of ```as``` when importing the library to shorten the call. We can do this by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before building a model, we want to explore the data first: some data cleaning, visualisation and simple statistics will be useful here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/nb-titanic.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to just get a quick glimpse of the data that we have loaded, we can just call ```data.head(n_rows)``` where ```n_rows``` is equal to the number of rows we want to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we feed our data into a classifier, we first have to do a bit of  manipulation to the ```DataFrame``` object. For the purposes of this practical we will convert much of the string data into categorical data. This is a fairly simple task in which we can leverage ```numpy``` to make things easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the irelevant variables\n",
    "data = data.drop(['name', 'ticket', 'cabin'], axis=1)\n",
    "\n",
    "# Fill in missing values with a mean\n",
    "age_mean = data['age'].mean()\n",
    "data['age'] = data['age'].fillna(age_mean)\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Fill in missing values with mode for discrete variables\n",
    "mode_embarked = mode(data['embarked'])[0][0]\n",
    "data['embarked'] = data['embarked'].fillna(mode_embarked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are only two unique values for the column Sex, we have no problems of ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'] = data['sex'].map({'female': 0, 'male': 1}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the column Embarked, however, replacing {C, S, Q} by {1, 2, 3} would seem to imply the ordering C < S < Q when in fact they are simply arranged alphabetically.\n",
    "\n",
    "To avoid this problem, we create dummy variables. Essentially this involves creating new columns to represent whether the passenger embarked at C with the value 1 if true, 0 otherwise. Pandas has a built-in function to create these columns automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data['embarked'], prefix='embarked').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, pd.get_dummies(data['embarked'], prefix='embarked')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "Write the code to create dummy variables for the column Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['sex', 'embarked'], axis=1)\n",
    "\n",
    "# Put column name to a list\n",
    "cols = data.columns.tolist()\n",
    "\n",
    "# Reoder the column names and the dataframe (data) according the new column order\n",
    "cols = [cols[1]] + cols[0:1] + cols[2:]\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We review our processed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the dataset: descriptive statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the data\n",
    "Data visualisation can be performed using Pandas and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline: To make matplotlib inline graphics\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for checking the distributions of the variables.\n",
    "data.survived.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data[\"survived\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'].plot(kind='hist') # Histogram for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots to compare the distribution of continuous variables by groups\n",
    "data.boxplot(column='age', by='survived')\n",
    "data.boxplot(column='fare', by='survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "# Visualise the data by groups in colors\n",
    "df0=data[data['survived']==0] # subset of data\n",
    "df1=data[data['survived']==1] # subset of data\n",
    "ax = df0.plot(kind='scatter', x='age', y='fare', color='green', label='survived')\n",
    "df1.plot(kind='scatter', x='age', y='fare', color='red', label='Not Survived', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "What are the other variables that you would like to visualise in order to understand the association between those variables and survival data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answer or code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the code above, analyse the column definitions and determine what features you would like the NB classifier to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.values[:,1:] # remember to exclude the output column (the first column here)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check to see whether or not this data has been set up correctly by ensuring that there are a equal amount of samples in both ```X``` and ```y```. If this throws and exception, alter your code to make it work. If you are still stuck, call over a demonstrator to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if X.shape[0] != y.shape[0]:\n",
    "    raise Exception(\"Sample counts do not align! Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setup your classifier\n",
    "\n",
    "## Decision Tree classifiers\n",
    "\n",
    "Ensemble learning is the technique of building multiple models to train, and then combining them in a manner that is likely to produce better results than individual models. These models don't have to be classifiers, and can be trained to deal with most tasks. A decision tree is a strcuture that, as the name alludes to, split the data into branches and make simple decisions at each level. From this, we are able to arrive at the final output by walking down the tree. The figure below is a simplistic decision tree that attempts to determine whether or not it is raining using features taken from a weather machine:\n",
    "\n",
    "![Simple Decision Tree](images/dt.png)\n",
    "**Figure Two:** A simplistic decision tree that attempts to determine whether or not it is raining using features taken from a weather machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a decision tree classifier and ```fit``` your data on it. If you are struggling to do this, take a look at the pseudocode:\n",
    "\n",
    "```python3\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(inputs, labels)\n",
    "```\n",
    "\n",
    "Splitting the data using train_test_split to accurately evaluate our models (a 80/20 split will suffice)\n",
    "Creating a Decision Tree classifier, training it using the training dataset (see https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "Evaluating model performance against the testing datasets\n",
    "Evaluate the model using the decision surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How did it perform? Was it good? Was it bad?\n",
    "\n",
    "The model is probably badly overfitted, and as such is unlikely to be a good general classification model. Just to prove this point, run the following code to split data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Clone your classifier, with default parameters.\n",
    "clf = clone(clf)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Just get a classification report.\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance\n",
    "\n",
    "It's important that we evaluate our model to see if it's a capable classifier. To do this we can make use of a number of metrics. Take a look at the ```sklearn.metrics``` documentation, and study what sort of metrics are suited for this task:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "Once you have done that, read the following pseudocode and try to evaluate your model yourself:\n",
    "\n",
    "```python3\n",
    "from sklearn.metrics import foo_score\n",
    "\n",
    "clf = Classifier()\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "foo = foo_score(y, y_pred)\n",
    "\n",
    "print(\"Foo Score: %f\" % (foo))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did it perform? Was it good? Was it bad?\n",
    "\n",
    "I'm afraid to say that the model is probably badly overfitted, and as such is unlikely to be a good general classification model. Just to prove this point, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Clone your classifier, with default parameters.\n",
    "clf = clone(clf)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Just get a classification report.\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about this right now if the model was bad. If you have time, you can play around with this at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your classifier \n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercies:\n",
    "\n",
    "Are there any parameters in the classifier that can be changed? In particular have a look at 'criterion', how about changing it to information gain? Look up the documentation to see how to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Different decision tree with alternative model configuration\n",
    "# You code here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "\n",
    "### Pandas Cheatsheet\n",
    "\n",
    "![Pandas Cheatsheet](images/pandascheat.png)\n",
    "\n",
    "### List slicing tips\n",
    "\n",
    "If you're new to the python programming language, understanding list slicing may be a bit difficult. Here's a quick guide.\n",
    "\n",
    "Given the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [\"This\", \"is\", \"a\", \"list\", \"of\", \"strings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wanted to get the first element of that list, I'd simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wanted to get the last element of that list, I'd simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wanted to get everything after the first element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if I wanted to get everything before the last element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, everything after the first and before the last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[1:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
