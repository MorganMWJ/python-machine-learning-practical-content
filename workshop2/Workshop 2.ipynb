{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests, the definition of insanity or an ingenious solution?\n",
    "\n",
    "### By Keiron O'Shea, and Chuan Lu\n",
    "\n",
    "![title](images/forest.jpeg)\n",
    "\n",
    "A Random Forest is another form of ensemble learning in which individual decision trees are created. This forest of Decision Trees is then used to predict the output value. As to esnure diversity among various trees, a random subset of training data is taken to construct each Decision Tree.\n",
    "\n",
    "## Overfitting, the data scientist's mortal enemy\n",
    "\n",
    "This method has an added bonus of not overfitting. We've referred to overfitting a couple times now, but what exactly is it - and why should we care? An overfit model can be described as being \"too complicated\" for the dataset. When this happens our model becomes too tailored to fit the random noise in our dataset - rather than being generalistic enough to model the likely inputs. In other words, our model should not fit only to the current set of training samples - but new samples too.\n",
    "\n",
    "<img src=\"images/overfitting.png\" width=\"400\">\n",
    "\n",
    "\"The green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and it is likely to have a higher error rate on new unseen data, compared to the black line.\" - Taken from https://en.wikipedia.org/wiki/Overfitting\n",
    "\n",
    "During creation nodes are split successively and the best performing nodes are chosen to reduce the entropy at each level. These splits do not consider all of the features passed, instead selecting the best split among the subset of the features that are under consideration. Adding randomness tends to bias the entire forest, but variance decreases due to averaging.\n",
    "\n",
    "## Diagnosing Colon Cancer from Microarray data\n",
    "\n",
    "Over the past two decades, Microarray technology has attracted increasing interest in many academic communities. This breakthrough in short read sequencing technology promises a new insight into the mechanisms of life by providing a way to simultaneously measure the activities and interaction of thousands of genes. For example, obtaining genome-wide ex-pression data from cancerous tissues provides clues for cancer classification and accurate diagnostic tests. However cancer classification remains a challenge to computer scientists. The main difficulties lie in the nature of the microarray gene expression data, which suffers from high-dimensionality and high-levels of noise.\n",
    "\n",
    "![title](images/Microarray.gif)\n",
    "\n",
    "In this workshop, we have provided you with some Microarray data taken from an extensive study of patients suffering from varying stages of colon cancer (Zexuan Zhu, Y. S. Ong and M. Dash, “Markov Blanket-Embedded Genetic Algorithm for Gene Selection”, Pattern Recognition, Vol. 49, No. 11, 3236-3248, 2007).\n",
    "\n",
    "Before we start, you need to import **pandas as pd** and **json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your imports go here\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion from Weka ARFF to Pandas DataFrame\n",
    "\n",
    "Unfortunately, the data was provided in the form of a Weka ```ARFF``` file. Below you can find a quick hack to convert the ```ARFF``` data into a ```pandas DataFrame```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arff_file = open(\"./data/Colon.arff\", \"r\").readlines()\n",
    "\n",
    "columns = []\n",
    "data = []\n",
    "\n",
    "for index, line in enumerate(arff_file):\n",
    "    #collect all attributes\n",
    "    if line.startswith(\"@ATTRIBUTE\"):\n",
    "        columns.append(line.split(\" \")[1])\n",
    "    #collect all data for each attribute\n",
    "    if line.startswith(\"@DATA\"):\n",
    "        for d in arff_file[index+1:]:\n",
    "            data.extend([d.replace(\"\\n\", \"\").split(\",\")])\n",
    "        break\n",
    "\n",
    "colon_df = pd.DataFrame(data, columns=columns)\n",
    "# Drop Controls\n",
    "colon_df = colon_df[[x for x in columns if x != \"control\"]]\n",
    "\n",
    "# Gene Information\n",
    "with open(\"./data/gene.json\", \"r\") as infile:\n",
    "    gene_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that you understand how best to deal with \"unknown\" data. The only tips I will give you is that the index is a range of n length (where n is equal to the number of patients) and the final column contains the classification label. You can use the following code cell to have a play around with the ```DataFrame``` if you so please:\n",
    "\n",
    "**Note:** I've also added descriptions for all of the localised genes, and provided them in a python dictonary named ```gene_dict```. If you have time, use this file for further inference of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H55933</th>\n",
       "      <th>R39465</th>\n",
       "      <th>R39465</th>\n",
       "      <th>R39465</th>\n",
       "      <th>R39465</th>\n",
       "      <th>R85482</th>\n",
       "      <th>U14973</th>\n",
       "      <th>R02593</th>\n",
       "      <th>R02593</th>\n",
       "      <th>T51496</th>\n",
       "      <th>...</th>\n",
       "      <th>R70790</th>\n",
       "      <th>L11706</th>\n",
       "      <th>T90549</th>\n",
       "      <th>D17390</th>\n",
       "      <th>M33210</th>\n",
       "      <th>H18490</th>\n",
       "      <th>H40891</th>\n",
       "      <th>R77780</th>\n",
       "      <th>T49647</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.416</td>\n",
       "      <td>5468.2407</td>\n",
       "      <td>4263.4077</td>\n",
       "      <td>5468.2407</td>\n",
       "      <td>4263.4077</td>\n",
       "      <td>4064.9358</td>\n",
       "      <td>1997.893</td>\n",
       "      <td>5282.325</td>\n",
       "      <td>2365.2424</td>\n",
       "      <td>2169.72</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.9125</td>\n",
       "      <td>138.89874</td>\n",
       "      <td>88.2325</td>\n",
       "      <td>39.66786</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.6775</td>\n",
       "      <td>83.5225</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>Tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.254</td>\n",
       "      <td>6719.5293</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>6719.5293</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.159</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.907</td>\n",
       "      <td>2865.0388</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59</td>\n",
       "      <td>82.2375</td>\n",
       "      <td>85.03333</td>\n",
       "      <td>152.195</td>\n",
       "      <td>186.5675</td>\n",
       "      <td>44.4725</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.705</td>\n",
       "      <td>6970.3613</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>6970.3613</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.65</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1678</td>\n",
       "      <td>763.3075</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>...</td>\n",
       "      <td>82.715</td>\n",
       "      <td>31.1025</td>\n",
       "      <td>193.92</td>\n",
       "      <td>76.9725</td>\n",
       "      <td>224.62024</td>\n",
       "      <td>31.225</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.0925</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>Tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.534</td>\n",
       "      <td>5955.835</td>\n",
       "      <td>7823.534</td>\n",
       "      <td>5955.835</td>\n",
       "      <td>3975.5642</td>\n",
       "      <td>2002.6132</td>\n",
       "      <td>2130.543</td>\n",
       "      <td>969.8237</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.925</td>\n",
       "      <td>183.00626</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.71072</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52</td>\n",
       "      <td>49.9825</td>\n",
       "      <td>16.085</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3286</td>\n",
       "      <td>3694.45</td>\n",
       "      <td>3400.74</td>\n",
       "      <td>3694.45</td>\n",
       "      <td>3400.74</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.782</td>\n",
       "      <td>1568.1113</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.35953</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.8125</td>\n",
       "      <td>Tumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      H55933     R39465     R39465     R39465     R39465     R85482  \\\n",
       "0   8589.416  5468.2407  4263.4077  5468.2407  4263.4077  4064.9358   \n",
       "1   9164.254  6719.5293  4883.4487  6719.5293  4883.4487   3718.159   \n",
       "2   3825.705  6970.3613  5369.9688  6970.3613  5369.9688    4705.65   \n",
       "3  6246.4487   7823.534   5955.835   7823.534   5955.835  3975.5642   \n",
       "4  3230.3286    3694.45    3400.74    3694.45    3400.74  3463.5857   \n",
       "\n",
       "      U14973     R02593     R02593     T51496  ...    R70790    L11706  \\\n",
       "0   1997.893   5282.325  2365.2424    2169.72  ...  67.56125  259.9125   \n",
       "1  2015.2214   5569.907  2865.0388  3849.0588  ...  92.23875  96.27625   \n",
       "2  1166.5536  1572.1678   763.3075  1325.4025  ...    82.715   31.1025   \n",
       "3  2002.6132   2130.543   969.8237  1531.1425  ...  41.68375     5.925   \n",
       "4  2181.4202   2922.782  1568.1113  2069.2463  ...  76.60375    161.35   \n",
       "\n",
       "      T90549    D17390     M33210    H18490    H40891   R77780    T49647  \\\n",
       "0  138.89874   88.2325   39.66786  67.82875   75.6775  83.5225  28.70125   \n",
       "1     150.59   82.2375   85.03333   152.195  186.5675  44.4725  16.77375   \n",
       "2     193.92   76.9725  224.62024    31.225  42.65625  16.0925  15.15625   \n",
       "3  183.00626  74.52875   67.71072  48.33875     42.52  49.9825    16.085   \n",
       "4   61.70125  54.56375  223.35953  73.09875  57.59875  7.48875   31.8125   \n",
       "\n",
       "    class  \n",
       "0   Tumor  \n",
       "1  Normal  \n",
       "2   Tumor  \n",
       "3  Normal  \n",
       "4   Tumor  \n",
       "\n",
       "[5 rows x 2165 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colon_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are tasked with the following:\n",
    "\n",
    "- Split the data up into ```training``` and ```testing``` sets.\n",
    "- Create a Random Forest Classifier to fit the training data.\n",
    "- Evaluate the Random Forest Classifier using appropriate forms of metrics.\n",
    "\n",
    "Here are some links that should help guide you with this task:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- Take a look at hyperparameters such as ```n_estimators``` and ```min_samples_split``` to get a better understanding as to how Random Forests work.  \n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\n",
    "\n",
    "\n",
    "\n",
    "If you are struggling to do this, take a look back at last weeks workshop for assistance. If you're still unable to complete the task, feel free to call a demonstrator over for additional help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  1]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#split data frame into input(x) and labels(y)\n",
    "#data from first n-1 features/columns are input\n",
    "X = colon_df.values[:,:-1]\n",
    "#data from last feature/column are labels\n",
    "y = colon_df[\"class\"].values\n",
    "\n",
    "#check equal samples in x and y - sanity check\n",
    "if X.shape[0] != y.shape[0]:\n",
    "    raise Exception(\"Sample counts do not align! Try again!\")\n",
    "\n",
    "#split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#create classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#train classifier\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "#make predictions for the test set\n",
    "test_y_predictions = clf.predict(X_test)\n",
    "\n",
    "#get confusion matrix of model \n",
    "print(confusion_matrix(y_test, test_y_predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#accuracy = tp+tn / p+n\n",
    "print(accuracy_score(y_test, test_y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1.        , 0.90909091]), array([0.66666667, 1.        ]), array([0.8       , 0.95238095]), array([ 3, 10]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(precision_recall_fscore_support(y_test, test_y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now visualise the trees using the following method, please take your time to understand what it does and how it does it. If you want to hack at it, a suggestion would be to save the files in a directory - as opposed to struggle with the sheer size of it within the ```notebook```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-29bc4e140755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\"C:\\Program Files (x86)\\graphviz-2.38\\bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "#\"C:\\Program Files (x86)\\graphviz-2.38\\bin\"\n",
    "#import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + \"C:\\Program Files (x86)\\graphviz-2.38\\bin\"\n",
    "\n",
    "def visualise_forest_of_trees(classifier):\n",
    "    for index, tree in enumerate(classifier.estimators_):\n",
    "        dot = export_graphviz(tree, filled=True, rounded=True)\n",
    "        graph = graphviz.Source(dot)\n",
    "        display(SVG(graph.pipe(format=\"svg\")))\n",
    "        if index > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -call function\n",
    "visualise_forest_of_trees(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter selection using GridSearch\n",
    "\n",
    "According to Wikipedia;\n",
    "\n",
    "\"The traditional way of performing hyperparameter optimisation has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\"\n",
    "\n",
    "When tasked with building classifiers, it is not always possible to know what the best parameters are. It is not feasible to \"brue-force\" parameters manually, so we make use of grid search. Grid Search provides us the abilit to specify a range of values from which classifiers can be built and evaluated using to find out the best combintation of parameters.\n",
    "\n",
    "So, to do this - we will first set up a couple imports and variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# These aren't real metrics, you can try using something like accuracy if you're struggling to find \n",
    "# relevant documentation\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\"]\n",
    "\n",
    "parameters = {\"n_estimators\" : [1, 10, 50, 100], \"max_depth\" : [2, 4, 8]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the above carefully, look at the parameters dictonary and extend them.\n",
    "\n",
    "\n",
    "Using the following **pseudocode** as a base, write something that will detail the best performing parameters:\n",
    "\n",
    "```\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "for metric in metrics:\n",
    "    clf = GridSearchCV(Classifier, parameter_grid, cv=10, scoring=metric)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # There is no such thing as grid_scores. Please read relevant documentation and\n",
    "    # find how to get best scores/estimatores.\n",
    "    for params, avg_score, _ in clf.grid_scores_:\n",
    "            print(params, round(avg_score, 3)\n",
    "    \n",
    "    clf = clf.best_params_\n",
    "    # Get predicted values\n",
    "    classification_report(y_test, y_pred)\n",
    "\n",
    "```\n",
    "\n",
    "Here's the documentation for GridSearchCV.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc = GridSearchCV(clf, parameters, cv=10, scoring=\"accuracy\")\n",
    "gsc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gsc.cv_results_)\n",
    "print(type(gsc.cv_results_.values()))\n",
    "pd.DataFrame(gsc.cv_results_.values(), columns=gsc.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gsc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mean_scores = [result.mean_validation_score for result in gsc.cv_results_]\n",
    "print(grid_mean_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time, try last weeks data (danio_rerio.csv) and see how the performance fares against last weeks benchmark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
